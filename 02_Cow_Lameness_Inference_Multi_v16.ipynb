{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ„ Cow Lameness Inference - Multi-Cow (v16)\n",
    "**Production & Clinical Reporting**\n",
    "\n",
    "## Objective\n",
    "Apply the trained **Tri-Modal Lameness Model** to multi-cow videos and generate clinical reports.\n",
    "\n",
    "## Pipeline\n",
    "1. **DeepLabCut**: Extract pose from entire video\n",
    "2. **YOLO + ByteTrack**: Detect and track individual cows\n",
    "3. **Tri-Modal Classification**: Fuse Pose + VideoMAE + RAFT for each cow\n",
    "4. **SAM Visualization**: Overlay colored masks (Red=Lame, Green=Healthy)\n",
    "5. **Clinical Report**: Export CSV with diagnosis per cow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q ultralytics supervision\n",
    "!pip install -q timm einops transformers\n",
    "!pip install -q \"deeplabcut[tf]\"\n",
    "!pip install -q segment-anything\n",
    "!pip install -q moviepy scikit-learn scipy\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phase 1: DeepLabCut Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import deeplabcut\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "DLC_PROJECT_NAME = \"CowGaitAnalysis_Inf\"\n",
    "DLC_OWNER = \"Researcher\"\n",
    "DLC_WORK_DIR = \"/content/dlc_work\"\n",
    "os.makedirs(DLC_WORK_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    config_path = deeplabcut.create_pretrained_project(\n",
    "        DLC_PROJECT_NAME, DLC_OWNER, [\"/content/drive/MyDrive/Raw_MultiCow_Videos/test_video.mp4\"], \n",
    "        working_directory=DLC_WORK_DIR, copy_videos=True, analyzevideo=False, \n",
    "        model=\"superanimal_quadruped\", videotype=\".mp4\"\n",
    "    )\n",
    "except:\n",
    "    search = glob.glob(f\"{DLC_WORK_DIR}/{DLC_PROJECT_NAME}*/config.yaml\")\n",
    "    config_path = search[0]\n",
    "\n",
    "INPUT_VIDEO = \"/content/drive/MyDrive/Raw_MultiCow_Videos/test_video.mp4\"\n",
    "TEMP_VIDEO = f\"/content/temp_inf_{os.path.basename(INPUT_VIDEO)}\"\n",
    "\n",
    "if not os.path.exists(TEMP_VIDEO):\n",
    "    shutil.copy(INPUT_VIDEO, TEMP_VIDEO)\n",
    "\n",
    "print(f\"Running DeepLabCut on {TEMP_VIDEO}...\")\n",
    "deeplabcut.analyze_videos(config_path, [TEMP_VIDEO], save_as_csv=False, destfolder=\"/content\")\n",
    "h5_files = glob.glob(f\"/content/temp_inf_*.h5\")\n",
    "dlc_data_path = h5_files[0]\n",
    "print(f\"âœ… DLC Complete: {dlc_data_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEModel\n",
    "from torchvision.models.optical_flow import raft_large, Raft_Large_Weights\n",
    "import torchvision.transforms.functional as F\n",
    "from collections import deque\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# A. YOLO for Detection\n",
    "yolo_model = YOLO(\"yolov8x.pt\")\n",
    "print(\"âœ… YOLO Loaded\")\n",
    "\n",
    "# B. SAM for Segmentation\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "sam.to(device=device)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "print(\"âœ… SAM Loaded\")\n",
    "\n",
    "# C. VideoMAE\n",
    "mae_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "mae_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device).eval()\n",
    "print(\"âœ… VideoMAE Loaded\")\n",
    "\n",
    "# D. RAFT\n",
    "raft_weights = Raft_Large_Weights.DEFAULT\n",
    "raft_model = raft_large(weights=raft_weights).to(device).eval()\n",
    "raft_tf = raft_weights.transforms()\n",
    "print(\"âœ… RAFT Loaded\")\n",
    "\n",
    "# E. Load DLC Data\n",
    "df_dlc = pd.read_hdf(dlc_data_path)\n",
    "scorer = df_dlc.columns.levels[0][0]\n",
    "bodyparts = df_dlc.columns.levels[1]\n",
    "dlc_matrix = df_dlc[scorer].values \n",
    "num_kpts = len(bodyparts)\n",
    "dlc_matrix = dlc_matrix.reshape(-1, num_kpts, 3)\n",
    "print(f\"âœ… DLC Data Loaded: {num_kpts} keypoints\")\n",
    "\n",
    "def get_dlc_points_for_box(frame_idx, box):\n",
    "    if frame_idx >= len(dlc_matrix): return np.zeros(num_kpts*3)\n",
    "    kpts = dlc_matrix[frame_idx] \n",
    "    valid = kpts[kpts[:,2] > 0.1]\n",
    "    if len(valid) == 0: return np.zeros(num_kpts*3)\n",
    "    avg_x, avg_y = np.mean(valid[:,0]), np.mean(valid[:,1])\n",
    "    x1, y1, x2, y2 = box\n",
    "    if x1 < avg_x < x2 and y1 < avg_y < y2:\n",
    "        return kpts.flatten()\n",
    "    return np.zeros(num_kpts*3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Trained Classification Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TriModalAttention(torch.nn.Module):\n",
    "    def __init__(self, pose_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.pose_proj = torch.nn.Linear(pose_dim, hidden_dim)\n",
    "        self.mae_proj = torch.nn.Linear(768, hidden_dim)\n",
    "        self.flow_proj = torch.nn.Linear(2, hidden_dim)\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=hidden_dim*3, nhead=4, batch_first=True, dropout=0.1)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim*3, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(64, 2)\n",
    "        )\n",
    "    def forward(self, p, m, f):\n",
    "        src = torch.cat([self.pose_proj(p), self.mae_proj(m), self.flow_proj(f)], dim=2)\n",
    "        out = self.encoder_layer(src)\n",
    "        return self.classifier(out.mean(dim=1))\n",
    "\n",
    "POS_DIM = num_kpts * 3\n",
    "gait_model = TriModalAttention(pose_dim=POS_DIM).to(device).eval()\n",
    "\n",
    "# Load Trained Weights with Validation\n",
    "WEIGHTS_PATH = \"/content/drive/MyDrive/outputs_v16_academic/cow_gait_transformer_v16_final.pth\"\n",
    "if os.path.exists(WEIGHTS_PATH):\n",
    "    checkpoint = torch.load(WEIGHTS_PATH, map_location=device)\n",
    "    \n",
    "    # Check if checkpoint contains metadata (new format) or just state_dict (legacy)\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        # New format with validation\n",
    "        expected_pose_dim = checkpoint['pose_dim']\n",
    "        \n",
    "        if POS_DIM != expected_pose_dim:\n",
    "            raise ValueError(\n",
    "                f\"âŒ DIMENSION MISMATCH!\\n\"\n",
    "                f\"Training used pose_dim={expected_pose_dim} (keypoints={expected_pose_dim//3})\\n\"\n",
    "                f\"Inference has pose_dim={POS_DIM} (keypoints={num_kpts})\\n\"\n",
    "                f\"Please ensure both notebooks use the same DeepLabCut model!\"\n",
    "            )\n",
    "        \n",
    "        gait_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"âœ… Model Loaded & Validated (pose_dim={expected_pose_dim})\")\n",
    "    else:\n",
    "        # Legacy format (just state_dict)\n",
    "        gait_model.load_state_dict(checkpoint)\n",
    "        print(\"âš ï¸ Model loaded (legacy format, no validation)\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: Model weights not found. Using untrained model!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_videomae_features(frames_list):\n",
    "    if not frames_list: return np.zeros(768)\n",
    "    indices = np.linspace(0, len(frames_list)-1, 16).astype(int)\n",
    "    sampled = [frames_list[i] for i in indices]\n",
    "    inputs = mae_processor(list(sampled), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = mae_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
    "\n",
    "def extract_raft_flow(frame1, frame2):\n",
    "    img1 = F.to_tensor(frame1).unsqueeze(0).to(device) * 255.0\n",
    "    img2 = F.to_tensor(frame2).unsqueeze(0).to(device) * 255.0\n",
    "    img1, img2 = raft_tf(img1, img2)\n",
    "    with torch.no_grad():\n",
    "        flow = raft_model(img1, img2)\n",
    "    return flow[-1].mean(dim=[2,3]).cpu().numpy()[0]\n",
    "\n",
    "class CowState:\n",
    "    def __init__(self, cow_id):\n",
    "        self.id = cow_id\n",
    "        self.crop_buffer = deque(maxlen=30)\n",
    "        self.pose_seq = deque(maxlen=30)\n",
    "        self.flow_seq = deque(maxlen=30)\n",
    "        self.predictions = []\n",
    "        self.last_crop = None\n",
    "        \n",
    "    def update(self, crop, pose_vec):\n",
    "        self.crop_buffer.append(crop)\n",
    "        self.pose_seq.append(pose_vec)\n",
    "        \n",
    "        if self.last_crop is not None:\n",
    "            h, w, _ = crop.shape\n",
    "            prev = cv2.resize(self.last_crop, (w, h))\n",
    "            f = extract_raft_flow(prev, crop)\n",
    "        else:\n",
    "            f = np.zeros(2)\n",
    "        self.flow_seq.append(f)\n",
    "        self.last_crop = crop\n",
    "        \n",
    "    def is_ready(self):\n",
    "        return len(self.pose_seq) == 30\n",
    "        \n",
    "    def predict(self):\n",
    "        if not self.is_ready(): return None\n",
    "        \n",
    "        m_vec = extract_videomae_features(list(self.crop_buffer))\n",
    "        \n",
    "        p_t = torch.tensor(np.array(self.pose_seq), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        f_t = torch.tensor(np.array(self.flow_seq), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        m_seq = np.tile(m_vec, (30, 1))\n",
    "        m_t = torch.tensor(m_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = gait_model(p_t, m_t, f_t)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "            \n",
    "        self.predictions.append(probs)\n",
    "        return probs\n",
    "\n",
    "print(\"âœ… Feature Extraction Ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracking & Classification Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "mask_annotator = sv.MaskAnnotator(opacity=0.5)\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "OUTPUT_VIDEO = \"/content/drive/MyDrive/outputs_v16_academic/inference_result_v16.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "width, height, fps = int(cap.get(3)), int(cap.get(4)), int(cap.get(5))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "cow_registry = {}\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\nProcessing Video: {total_frames} frames @ {fps} FPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=total_frames, desc=\"Inference Progress\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    frame_count += 1\n",
    "    \n",
    "    # Detection\n",
    "    results = yolo_model(frame, classes=[19], verbose=False)\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "    \n",
    "    # SAM Prep\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    sam_predictor.set_image(frame_rgb)\n",
    "    \n",
    "    masks_list = []\n",
    "    labels = []\n",
    "    \n",
    "    for xyxy, track_id in zip(detections.xyxy, detections.tracker_id):\n",
    "        # SAM Mask\n",
    "        sam_masks, _, _ = sam_predictor.predict(box=xyxy, multimask_output=False)\n",
    "        mask = sam_masks[0]\n",
    "        masks_list.append(mask)\n",
    "        \n",
    "        # Crop\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        crop = frame[y1:y2, x1:x2]\n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # DLC Pose\n",
    "        pose_vec = get_dlc_points_for_box(frame_count-1, xyxy)\n",
    "        \n",
    "        # Update State\n",
    "        if track_id not in cow_registry:\n",
    "            cow_registry[track_id] = CowState(track_id)\n",
    "        \n",
    "        cow = cow_registry[track_id]\n",
    "        cow.update(crop_rgb, pose_vec)\n",
    "        \n",
    "        # Classify\n",
    "        status = \"Analyzing...\"\n",
    "        if cow.is_ready() and frame_count % 5 == 0:\n",
    "            cow.predict()\n",
    "        \n",
    "        if cow.predictions:\n",
    "            avg = np.mean(cow.predictions[-10:], axis=0)\n",
    "            is_lame = avg[1] > 0.5\n",
    "            conf = max(avg)\n",
    "            status = f\"TOPAL {conf:.1%}\" if is_lame else f\"SAGLIKLI {conf:.1%}\"\n",
    "        \n",
    "        labels.append(f\"#{track_id} {status}\")\n",
    "    \n",
    "    # Annotate\n",
    "    if len(masks_list) > 0:\n",
    "        detections.mask = np.array(masks_list)\n",
    "    \n",
    "    annotated = mask_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "    annotated = label_annotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
    "    \n",
    "    out.write(annotated)\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Calculate Performance Metrics\n",
    "elapsed_time = time.time() - start_time\n",
    "processing_fps = frame_count / elapsed_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Video Processing Complete\")\n",
    "print(f\"ðŸ“Š Performance Metrics:\")\n",
    "print(f\"   Total Frames: {frame_count}\")\n",
    "print(f\"   Processing Time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"   Processing FPS: {processing_fps:.2f}\")\n",
    "print(f\"   Speedup: {processing_fps/fps:.2f}x realtime\")\n",
    "print(f\"   Cows Tracked: {len(cow_registry)}\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Clinical Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report_data = []\n",
    "\n",
    "for cid, state in cow_registry.items():\n",
    "    if not state.predictions:\n",
    "        status = \"Insufficient Data\"\n",
    "        score = 0.0\n",
    "    else:\n",
    "        avg = np.mean(state.predictions, axis=0)\n",
    "        is_lame = avg[1] > 0.5\n",
    "        status = \"TOPAL (LAME)\" if is_lame else \"SAGLIKLI (HEALTHY)\"\n",
    "        score = avg[1] if is_lame else avg[0]\n",
    "        \n",
    "    report_data.append({\n",
    "        'Cow_ID': cid,\n",
    "        'Diagnosis': status,\n",
    "        'Confidence': f\"{score:.4f}\",\n",
    "        'Frames_Tracked': len(state.predictions),\n",
    "        'Duration_Seconds': len(state.predictions) / fps\n",
    "    })\n",
    "\n",
    "df_report = pd.DataFrame(report_data)\n",
    "df_report.to_csv(\"/content/drive/MyDrive/outputs_v16_academic/clinical_report_v16.csv\", index=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Clinical Report:\")\n",
    "print(df_report.to_string())\n",
    "print(f\"\\nâœ… Report saved to: clinical_report_v16.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}