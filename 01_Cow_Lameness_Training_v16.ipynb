{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêÑ Cow Lameness Detection - Training & Research (v16)\n",
    "**Academic Gold Standard Edition - PRODUCTION**\n",
    "\n",
    "## Objective\n",
    "This notebook implements a state-of-the-art **Tri-Modal Gait Analysis** system to detect lameness in cows using DeepLabCut SuperAnimal.\n",
    "\n",
    "## Methodology\n",
    "1.  **Phase 1**: Run **DeepLabCut (SuperAnimal-Quadruped)** on ALL training videos\n",
    "2.  **Phase 2**: Extract Visual Features (VideoMAE, RAFT)\n",
    "3.  **Phase 3**: Biometric Statistical Analysis (T-Test)\n",
    "4.  **Phase 4**: Train with 5-Fold Cross-Validation\n",
    "5.  **Phase 5**: Explainable AI (Attention Heatmaps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Install Core Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install in stages to avoid dependency conflicts\n",
    "!pip install -q ultralytics supervision\n",
    "!pip install -q timm einops transformers\n",
    "!pip install -q moviepy scikit-learn scipy seaborn matplotlib\n",
    "!pip install -q psutil gputil\n",
    "print(\"‚úÖ Core dependencies installed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: DeepLabCut Setup (IMPORTANT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **‚ö†Ô∏è CRITICAL COMPATIBILITY ISSUE**\n",
    ">\n",
    "> DeepLabCut requires NumPy <2.0, but Colab now uses NumPy 2.2+.\n",
    "> Downgrading NumPy breaks many other Colab packages (opencv, jax, etc.).\n",
    ">\n",
    "> **RECOMMENDED SOLUTION**: Process DLC offline, upload CSVs to Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(\"üîÑ DEEPLABCUT WORKAROUND\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(\"\")\n",
    "print(\"Option A: SKIP DLC (Use Pre-Computed Results)\")\n",
    "print(\"  1. Run DLC SuperAnimal locally or on dedicated machine\")\n",
    "print(\"  2. Upload CSV files to Drive alongside videos\")\n",
    "print(\"  3. Notebook will auto-detect and use CSVs\")\n",
    "print(\"\")\n",
    "print(\"Option B: Try DLC Installation (May Fail)\")\n",
    "print(\"  - Uncomment the code below to attempt install\")\n",
    "print(\"  - Expect dependency conflicts\")\n",
    "print(\"\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "\n",
    "# UNCOMMENT BELOW TO ATTEMPT DLC INSTALL (NOT RECOMMENDED)\n",
    "# !pip install -q --force-reinstall numpy==1.26.4\n",
    "# !pip install -q deeplabcut --no-deps\n",
    "# !pip install -q dlclibrary filterpy ruamel.yaml imgaug scikit-image\n",
    "# import deeplabcut\n",
    "# print(f\"‚úÖ DLC {deeplabcut.__version__}\")\n",
    "\n",
    "# RECOMMENDED: Check if CSVs exist\n",
    "import os\n",
    "import glob\n",
    "BASE_DIR_CHECK = \"/content/drive/MyDrive/Inek Topallik Tespiti Parcalanmis Inek Videolari/cow_single_videos\"\n",
    "if os.path.exists(BASE_DIR_CHECK):\n",
    "    csv_files = glob.glob(f\"{BASE_DIR_CHECK}/**/*.csv\", recursive=True)\n",
    "    if csv_files:\n",
    "        print(f\"\\n‚úÖ Found {len(csv_files)} DLC CSV files in Drive!\")\n",
    "        print(\"   You can proceed without installing DLC.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No CSV files found. You need to:\")\n",
    "        print(\"   1. Run DLC SuperAnimal offline\")\n",
    "        print(\"   2. Upload '*DLC*.csv' files next to videos\")\n",
    "else:\n",
    "    print(\"\\nüìÅ Drive path not yet mounted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Mount Drive & Setup Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/Inek Topallik Tespiti Parcalanmis Inek Videolari/cow_single_videos\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/outputs_v16_academic\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "CLASSES = ['Saglikli', 'Topal']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Get List of All Videos\n",
    "all_videos = []\n",
    "for label in CLASSES:\n",
    "    folder = os.path.join(BASE_DIR, label)\n",
    "    vids = glob.glob(os.path.join(folder, \"*.mp4\"))\n",
    "    all_videos.extend(vids)\n",
    "    \n",
    "print(f\"Total Videos Found: {len(all_videos)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PHASE 1: DeepLabCut SuperAnimal Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **CRITICAL ACADEMIC STEP**: Running official `deeplabcut.analyze_videos` on entire dataset.\n",
    "> This uses **SuperAnimal-Quadruped** (Ye et al., 2024) for high-fidelity pose estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import deeplabcut\n",
    "\n",
    "print(\"Initializing DeepLabCut SuperAnimal-Quadruped...\")\n",
    "DLC_PROJECT_NAME = \"CowGaitAnalysis\"\n",
    "DLC_OWNER = \"Researcher\"\n",
    "DLC_WORK_DIR = \"/content/dlc_work\"\n",
    "os.makedirs(DLC_WORK_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize Project Configuration\n",
    "try:\n",
    "    dummy_vid = all_videos[0]\n",
    "    config_path = deeplabcut.create_pretrained_project(\n",
    "        DLC_PROJECT_NAME, DLC_OWNER, [dummy_vid], \n",
    "        working_directory=DLC_WORK_DIR, copy_videos=False, analyzevideo=False, \n",
    "        model=\"superanimal_quadruped\", videotype=\".mp4\"\n",
    "    )\n",
    "    print(f\"‚úÖ DLC Project Created: {config_path}\")\n",
    "except Exception as e:\n",
    "    # If already exists\n",
    "    search = glob.glob(f\"{DLC_WORK_DIR}/{DLC_PROJECT_NAME}*/config.yaml\")\n",
    "    config_path = search[0] if search else None\n",
    "    print(f\"üìÇ Using Existing DLC Config: {config_path}\")\n",
    "\n",
    "# RUN BATCH ANALYSIS with Time Estimation\n",
    "import time\n",
    "estimated_time_per_video = 2  # minutes (conservative estimate)\n",
    "total_estimated_minutes = len(all_videos) * estimated_time_per_video\n",
    "hours = total_estimated_minutes // 60\n",
    "minutes = total_estimated_minutes % 60\n",
    "\n",
    "print(f\"\\nüöÄ Starting Batch Analysis of {len(all_videos)} videos...\")\n",
    "print(f\"‚è∞ Estimated Time: ~{hours}h {minutes}m (may vary based on video length)\")\n",
    "print(\"‚è≥ Please be patient, this is a one-time process...\")\n",
    "print(\"üí° TIP: Results are cached in Drive. Subsequent runs will skip processed videos.\")\n",
    "\n",
    "start_dlc = time.time()\n",
    "deeplabcut.analyze_videos(\n",
    "    config_path, \n",
    "    all_videos, \n",
    "    videotype='.mp4', \n",
    "    save_as_csv=True, \n",
    "    destfolder=None  # Save next to video file\n",
    ")\n",
    "elapsed_dlc = (time.time() - start_dlc) / 60\n",
    "print(f\"‚úÖ DeepLabCut Complete! Actual time: {elapsed_dlc:.1f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PHASE 2: Visual Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "from torchvision.models.optical_flow import raft_large, Raft_Large_Weights\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEModel\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# A. VideoMAE\n",
    "mae_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "mae_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device).eval()\n",
    "\n",
    "# B. RAFT\n",
    "raft_weights = Raft_Large_Weights.DEFAULT\n",
    "raft_transforms = raft_weights.transforms()\n",
    "raft_model = raft_large(weights=raft_weights, progress=False).to(device).eval()\n",
    "\n",
    "# C. YOLO (for cropping)\n",
    "yolo_model = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "def extract_videomae_features(frames_list):\n",
    "    if not frames_list: return np.zeros(768)\n",
    "    indices = np.linspace(0, len(frames_list)-1, 16).astype(int)\n",
    "    sampled = [frames_list[i] for i in indices]\n",
    "    inputs = mae_processor(list(sampled), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = mae_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
    "\n",
    "def extract_raft_flow(frame1, frame2):\n",
    "    img1 = F.to_tensor(frame1).unsqueeze(0).to(device) * 255.0\n",
    "    img2 = F.to_tensor(frame2).unsqueeze(0).to(device) * 255.0\n",
    "    img1, img2 = raft_transforms(img1, img2)\n",
    "    with torch.no_grad():\n",
    "        flow_predictions = raft_model(img1, img2)\n",
    "    return flow_predictions[-1].mean(dim=[2,3]).cpu().numpy()[0]\n",
    "\n",
    "print(\"‚úÖ Visual Feature Engines Loaded\")\n",
    "\n",
    "# Memory Monitoring for Colab Pro+\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "ram_percent = psutil.virtual_memory().percent\n",
    "print(f\"\\nüìä System RAM Usage: {ram_percent:.1f}%\")\n",
    "\n",
    "try:\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        gpu = gpus[0]\n",
    "        print(f\"üéÆ GPU RAM Usage: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è GPU monitoring not available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Fusion (DLC + VideoMAE + RAFT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video_fusion(video_path):\n",
    "    # 1. Load DLC CSV\n",
    "    folder = os.path.dirname(video_path)\n",
    "    base = os.path.basename(video_path).replace('.mp4','')\n",
    "    candidates = glob.glob(os.path.join(folder, f\"*{base}*.csv\"))\n",
    "    \n",
    "    dlc_csv = None\n",
    "    for c in candidates:\n",
    "        if \"DLC\" in c or \"superanimal\" in c.lower():\n",
    "            dlc_csv = c\n",
    "            break\n",
    "    \n",
    "    if not dlc_csv:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(dlc_csv, header=[1,2])\n",
    "        pose_raw = df.values\n",
    "        pose_raw = np.nan_to_num(pose_raw, nan=0.0)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # 2. Load Video Frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) < 10: return None\n",
    "    \n",
    "    # Resample to 30 frames\n",
    "    indices = np.linspace(0, len(frames)-1, 30).astype(int)\n",
    "    frames = [frames[i] for i in indices]\n",
    "    pose_indices = np.linspace(0, len(pose_raw)-1, 30).astype(int)\n",
    "    pose_seq = pose_raw[pose_indices]\n",
    "    \n",
    "    # 3. Extract Visual Features\n",
    "    cropped_frames = []\n",
    "    flow_seq = []\n",
    "    last_crop = None\n",
    "    \n",
    "    for frame in frames:\n",
    "        # YOLO Crop\n",
    "        results = yolo_model(frame, classes=[19], verbose=False)\n",
    "        if results and len(results[0].boxes) > 0:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            best_box = boxes[np.argmax([(b[2]-b[0])*(b[3]-b[1]) for b in boxes])]\n",
    "            x1,y1,x2,y2 = map(int, best_box)\n",
    "            crop = frame[y1:y2, x1:x2]\n",
    "        else:\n",
    "            crop = frame\n",
    "            \n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        cropped_frames.append(crop_rgb)\n",
    "        \n",
    "        # Flow\n",
    "        if last_crop is not None:\n",
    "            h, w, _ = crop_rgb.shape\n",
    "            prev_resized = cv2.resize(last_crop, (w, h))\n",
    "            f = extract_raft_flow(prev_resized, crop_rgb)\n",
    "        else:\n",
    "            f = np.zeros(2)\n",
    "        flow_seq.append(f)\n",
    "        last_crop = crop_rgb\n",
    "\n",
    "    mae_global = extract_videomae_features(cropped_frames)\n",
    "    mae_seq = np.tile(mae_global, (30, 1))\n",
    "    \n",
    "    return {'pose': pose_seq, 'mae': mae_seq, 'flow': np.array(flow_seq)}\n",
    "\n",
    "# Build Final Dataset\n",
    "data_records = []\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\nüîÑ Processing videos and extracting features...\")\n",
    "for vid in tqdm(all_videos, desc=\"Feature Extraction\"):\n",
    "    label = 1 if 'Topal' in vid else 0\n",
    "    try:\n",
    "        feats = process_video_fusion(vid)\n",
    "        if feats:\n",
    "            data_records.append({\n",
    "                'video': os.path.basename(vid),\n",
    "                'label': label,\n",
    "                'pose': feats['pose'],\n",
    "                'mae': feats['mae'],\n",
    "                'flow': feats['flow']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error {vid}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Final Dataset: {len(data_records)} samples\")\n",
    "\n",
    "# Check RAM after feature extraction\n",
    "ram_after = psutil.virtual_memory().percent\n",
    "print(f\"üìä RAM Usage after features: {ram_after:.1f}%\")\n",
    "if ram_after > 80:\n",
    "    print(\"‚ö†Ô∏è WARNING: High RAM usage detected. Consider clearing variables if needed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PHASE 3: Biometric Statistical Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def get_back_curvature_angle(pose_seq):\n",
    "    kpts = pose_seq.reshape(30, -1, 3).mean(axis=0)\n",
    "    if kpts.shape[0] < 15: return 180\n",
    "    return calculate_angle(kpts[5,:2], kpts[10,:2], kpts[15,:2])\n",
    "\n",
    "healthy_scores = [get_back_curvature_angle(d['pose']) for d in data_records if d['label'] == 0]\n",
    "lame_scores = [get_back_curvature_angle(d['pose']) for d in data_records if d['label'] == 1]\n",
    "\n",
    "if len(healthy_scores) > 0 and len(lame_scores) > 0:\n",
    "    t_stat, p_val = stats.ttest_ind(healthy_scores, lame_scores)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(healthy_scores, fill=True, label='Saglikli (Healthy)', color='green', alpha=0.6)\n",
    "    sns.kdeplot(lame_scores, fill=True, label='Topal (Lame)', color='red', alpha=0.6)\n",
    "    plt.title(f\"Biometric Validation: Back Spine Angle\\n(Lower=More Curvature) | p-value={p_val:.4e}\", fontsize=14)\n",
    "    plt.xlabel(\"Spine Angle (degrees)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/biometric_significance.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Statistical Significance: p={p_val:.4e} {'(SIGNIFICANT ‚úì)' if p_val < 0.05 else '(NOT SIGNIFICANT)'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PHASE 4: Model Training with 5-Fold CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class TriModalAttention(nn.Module):\n",
    "    def __init__(self, pose_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.pose_proj = nn.Linear(pose_dim, hidden_dim)\n",
    "        self.mae_proj = nn.Linear(768, hidden_dim)\n",
    "        self.flow_proj = nn.Linear(2, hidden_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim*3, nhead=4, batch_first=True, dropout=0.1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pose, mae, flow):\n",
    "        src = torch.cat([self.pose_proj(pose), self.mae_proj(mae), self.flow_proj(flow)], dim=2)\n",
    "        out = self.encoder_layer(src)\n",
    "        pooled = out.mean(dim=1)\n",
    "        return self.classifier(pooled), pooled\n",
    "\n",
    "class CowDataset(Dataset):\n",
    "    def __init__(self, records):\n",
    "        self.records = records\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.records[i]\n",
    "        return (torch.tensor(r['pose'], dtype=torch.float32), \n",
    "                torch.tensor(r['mae'], dtype=torch.float32), \n",
    "                torch.tensor(r['flow'], dtype=torch.float32), \n",
    "                torch.tensor(r['label'], dtype=torch.long))\n",
    "\n",
    "if len(data_records) == 0:\n",
    "    raise RuntimeError(\"No data loaded! Check DLC CSVs.\")\n",
    "\n",
    "sample_pose_dim = data_records[0]['pose'].shape[1]\n",
    "print(f\"Pose Dimension: {sample_pose_dim}\")\n",
    "\n",
    "# CRITICAL: Train/Test Split for Unbiased Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = [r['label'] for r in data_records]\n",
    "train_records, test_records = train_test_split(\n",
    "    data_records, \n",
    "    test_size=0.2,  # 80% train, 20% test\n",
    "    stratify=labels,  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET SPLIT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Samples: {len(data_records)}\")\n",
    "print(f\"Training Set: {len(train_records)} samples\")\n",
    "print(f\"Test Set (HELD-OUT): {len(test_records)} samples\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"üìå Test set will ONLY be used for final evaluation after training.\")\n",
    "\n",
    "# 5-Fold Cross-Validation on TRAINING SET ONLY\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_labels = [r['label'] for r in train_records]\n",
    "\n",
    "fold_results = []\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_probs = []  # For ROC-AUC\n",
    "train_losses_per_fold = []\n",
    "val_losses_per_fold = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_records, train_labels)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold+1}/5\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_data = [train_records[i] for i in train_idx]\n",
    "    val_data = [train_records[i] for i in val_idx]\n",
    "    \n",
    "    train_loader = DataLoader(CowDataset(train_data), batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(CowDataset(val_data), batch_size=8, shuffle=False)\n",
    "    \n",
    "    model = TriModalAttention(pose_dim=sample_pose_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training with Loss Tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for p, m, f, y in train_loader:\n",
    "            p, m, f, y = p.to(device), m.to(device), f.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(p, m, f)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation Loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for p, m, f, y in val_loader:\n",
    "                p, m, f, y = p.to(device), m.to(device), f.to(device), y.to(device)\n",
    "                logits, _ = model(p, m, f)\n",
    "                val_loss += criterion(logits, y).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/20 | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Final Validation with Probabilities\n",
    "    model.eval()\n",
    "    preds, trues, embeds, probs_list = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for p, m, f, y in val_loader:\n",
    "            p, m, f, y = p.to(device), m.to(device), f.to(device), y.to(device)\n",
    "            logits, emb = model(p, m, f)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            trues.extend(y.cpu().numpy())\n",
    "            embeds.extend(emb.cpu().numpy())\n",
    "            probs_list.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (Lame)\n",
    "    \n",
    "    acc = accuracy_score(trues, preds)\n",
    "    fold_results.append(acc)\n",
    "    all_embeddings.extend(embeds)\n",
    "    all_labels.extend(trues)\n",
    "    all_probs.extend(probs_list)\n",
    "    train_losses_per_fold.append(train_losses)\n",
    "    val_losses_per_fold.append(val_losses)\n",
    "    \n",
    "    print(f\"‚úÖ Fold {fold+1} Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(f\"\n",
    "{'='*50}\")\n",
    "print(f\"5-FOLD CV RESULTS (Training Set Only)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean Accuracy: {np.mean(fold_results):.4f} ¬± {np.std(fold_results):.4f}\")\n",
    "\n",
    "# Final Test Set Evaluation (UNBIASED)\n",
    "print(f\"\n",
    "{'='*60}\")\n",
    "print(f\"FINAL TEST EVALUATION (HELD-OUT SET)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "test_loader = DataLoader(CowDataset(test_records), batch_size=8, shuffle=False)\n",
    "\n",
    "# Use best model (last fold for simplicity, ideally use best val accuracy model)\n",
    "model.eval()\n",
    "test_preds, test_trues, test_probs = [], [], []\n",
    "with torch.no_grad():\n",
    "    for p, m, f, y in test_loader:\n",
    "        p, m, f, y = p.to(device), m.to(device), f.to(device), y.to(device)\n",
    "        logits, _ = model(p, m, f)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        test_trues.extend(y.cpu().numpy())\n",
    "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_trues, test_preds)\n",
    "print(f\"‚úÖ Test Set Accuracy: {test_acc:.4f}\")\n",
    "print(f\"üìä Test Set Size: {len(test_trues)} samples\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"üéØ This is the FINAL, UNBIASED performance estimate for publication.\")\n",
    "\n",
    "# Save Final Model with Metadata for Validation\n",
    "model_checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'pose_dim': sample_pose_dim,\n",
    "    'mae_dim': 768,\n",
    "    'flow_dim': 2,\n",
    "    'test_accuracy': test_acc  # Include test performance\n",
    "}\n",
    "torch.save(model_checkpoint, f\"{OUTPUT_DIR}/cow_gait_transformer_v16_final.pth\")\n",
    "print(f\"\n",
    "‚úÖ Model Saved with Metadata (pose_dim={sample_pose_dim}, test_acc={test_acc:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Training Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot Loss Curves (Average across folds)\n",
    "avg_train_losses = np.mean(train_losses_per_fold, axis=0)\n",
    "avg_val_losses = np.mean(val_losses_per_fold, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, avg_train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs, avg_val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training & Validation Loss Curves (5-Fold Average)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{OUTPUT_DIR}/loss_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"‚úÖ Loss Curves Saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. ROC-AUC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC-AUC Curve (All Folds Combined)', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{OUTPUT_DIR}/roc_auc_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ ROC-AUC: {roc_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PHASE 5: t-SNE Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t-SNE of Feature Space\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(np.array(all_embeddings))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, color, name in [(0, 'green', 'Healthy'), (1, 'red', 'Lame')]:\n",
    "    idx = np.array(all_labels) == label\n",
    "    plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1], \n",
    "                c=color, label=name, alpha=0.6, s=50)\n",
    "\n",
    "plt.title(\"t-SNE: Feature Space Separation (Healthy vs Lame)\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{OUTPUT_DIR}/tsne_clusters.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"‚úÖ t-SNE Visualization Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix & Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cm = confusion_matrix(all_labels, [1 if e[1] > e[0] else 0 for e in all_embeddings[:len(all_labels)]])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Healthy', 'Lame'], \n",
    "            yticklabels=['Healthy', 'Lame'])\n",
    "plt.title(\"Confusion Matrix (All Folds Combined)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/confusion_matrix.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report([all_labels[i] for i in range(len(preds))], \n",
    "                           preds, \n",
    "                           target_names=['Healthy', 'Lame']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ablation Study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Academic Validation**: Compare individual modalities vs. fusion to prove superiority of Tri-Modal approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train 3 Models: (A) Pose-Only, (B) VideoMAE-Only, (C) Tri-Modal (Ours)\n",
    "\n",
    "class PoseOnlyModel(nn.Module):\n",
    "    def __init__(self, pose_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=pose_dim, nhead=4, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.Linear(pose_dim, 64), nn.ReLU(), nn.Linear(64, 2))\n",
    "    def forward(self, pose):\n",
    "        out = self.encoder(pose)\n",
    "        return self.classifier(out.mean(dim=1))\n",
    "\n",
    "class VideoMAEOnlyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=768, nhead=8, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.Linear(768, 64), nn.ReLU(), nn.Linear(64, 2))\n",
    "    def forward(self, mae):\n",
    "        out = self.encoder(mae)\n",
    "        return self.classifier(out.mean(dim=1))\n",
    "\n",
    "def train_ablation_model(model, data_records, model_name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    labels = [r['label'] for r in data_records]\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_accs = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(data_records, labels)):\n",
    "        train_data = [data_records[i] for i in train_idx]\n",
    "        val_data = [data_records[i] for i in val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(CowDataset(train_data), batch_size=8, shuffle=True)\n",
    "        val_loader = DataLoader(CowDataset(val_data), batch_size=8, shuffle=False)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Quick Training (10 epochs for ablation)\n",
    "        for epoch in range(10):\n",
    "            model.train()\n",
    "            for p, m, f, y in train_loader:\n",
    "                p, m, f, y = p.to(device), m.to(device), f.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if model_name == \"Pose-Only\":\n",
    "                    logits = model(p)\n",
    "                elif model_name == \"VideoMAE-Only\":\n",
    "                    logits = model(m)\n",
    "                else:  # Tri-Modal\n",
    "                    logits, _ = model(p, m, f)\n",
    "                \n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for p, m, f, y in val_loader:\n",
    "                p, m, f, y = p.to(device), m.to(device), f.to(device), y.to(device)\n",
    "                \n",
    "                if model_name == \"Pose-Only\":\n",
    "                    logits = model(p)\n",
    "                elif model_name == \"VideoMAE-Only\":\n",
    "                    logits = model(m)\n",
    "                else:\n",
    "                    logits, _ = model(p, m, f)\n",
    "                \n",
    "                preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                trues.extend(y.cpu().numpy())\n",
    "        \n",
    "        acc = accuracy_score(trues, preds)\n",
    "        fold_accs.append(acc)\n",
    "    \n",
    "    mean_acc = np.mean(fold_accs)\n",
    "    std_acc = np.std(fold_accs)\n",
    "    print(f\"‚úÖ {model_name}: {mean_acc:.4f} ¬± {std_acc:.4f}\")\n",
    "    return mean_acc, std_acc\n",
    "\n",
    "# Run Ablation\n",
    "ablation_results = {}\n",
    "\n",
    "model_a = PoseOnlyModel(sample_pose_dim).to(device)\n",
    "ablation_results['Pose-Only'] = train_ablation_model(model_a, data_records, \"Pose-Only\")\n",
    "\n",
    "model_b = VideoMAEOnlyModel().to(device)\n",
    "ablation_results['VideoMAE-Only'] = train_ablation_model(model_b, data_records, \"VideoMAE-Only\")\n",
    "\n",
    "model_c = TriModalAttention(pose_dim=sample_pose_dim).to(device)\n",
    "ablation_results['Tri-Modal (Ours)'] = train_ablation_model(model_c, data_records, \"Tri-Modal (Ours)\")\n",
    "\n",
    "# Plot Results\n",
    "models = list(ablation_results.keys())\n",
    "means = [ablation_results[m][0] for m in models]\n",
    "stds = [ablation_results[m][1] for m in models]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, means, yerr=stds, capsize=10, alpha=0.7, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Ablation Study: Contribution of Each Modality', fontsize=14)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{mean:.3f}¬±{std:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.savefig(f\"{OUTPUT_DIR}/ablation_study.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for model_name, (mean, std) in ablation_results.items():\n",
    "    print(f\"{model_name:20s}: {mean:.4f} ¬± {std:.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Ablation Study Complete\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}